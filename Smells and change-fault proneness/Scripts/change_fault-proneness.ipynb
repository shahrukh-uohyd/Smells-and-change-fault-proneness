{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed files using pydriller\n",
    "import os\n",
    "import csv\n",
    "from pydriller import Repository\n",
    "from git import Repo, GitCommandError\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "def detect_fault_inducing_commits(repo_path, release_pairs, output_dir=\"buggy_files_reports\"):\n",
    "    \"\"\"\n",
    "    Enhanced fault-inducing commit detection with better blame handling and debugging.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    git_repo = Repo(repo_path)\n",
    "    results = {}\n",
    "    project_name = repo_path.split(\"/\")[-1]\n",
    "    if project_name==\"vlc-android\":\n",
    "        project_name=\"vlc\"\n",
    "\n",
    "    for start_release, end_release in release_pairs:\n",
    "        try:\n",
    "            print(f\"\\nAnalyzing {start_release} to {end_release}\")\n",
    "            \n",
    "            # Get commit range\n",
    "            start_commit = git_repo.tags[start_release].commit\n",
    "            end_commit = git_repo.tags[end_release].commit\n",
    "            \n",
    "            csv_filename = os.path.join(output_dir, f\"sqlite-{start_release}.csv\")\n",
    "            buggy_files = set()\n",
    "            fix_count = 0\n",
    "            \n",
    "            with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "                    'buggy_file_path', 'fix_commit_hash', 'fix_date',\n",
    "                    'buggy_commit_hash', 'buggy_date', 'fix_message'\n",
    "                ])\n",
    "                writer.writeheader()\n",
    "                \n",
    "                # Traverse commits with PyDriller\n",
    "                for commit in Repository(\n",
    "                    repo_path,\n",
    "                    since=start_commit.committed_datetime,\n",
    "                    to=end_commit.committed_datetime,\n",
    "                    only_modifications_with_file_types=['.java', '.cpp', '.c', '.h']  # Filter by file type\n",
    "                ).traverse_commits():\n",
    "                    \n",
    "#                     if not is_fix_commit(commit.msg):\n",
    "#                         continue\n",
    "                    \n",
    "                    fix_count += 1\n",
    "                    print(f\"  Fix commit: {commit.hash[:8]} - {commit.msg[:50]}...\")\n",
    "                    \n",
    "                    for modified_file in commit.modified_files:\n",
    "                        if not modified_file.new_path:\n",
    "                            continue\n",
    "                            \n",
    "                        try:\n",
    "                            # Get previous version of the file\n",
    "                            old_path = modified_file.old_path or modified_file.new_path\n",
    "                            previous_contents = git_repo.git.show(f\"{commit.hash}^:{old_path}\")\n",
    "                            current_contents = git_repo.git.show(f\"{commit.hash}:{modified_file.new_path}\")\n",
    "                            \n",
    "                            # Get changed lines\n",
    "                            diff = get_changed_lines(previous_contents, current_contents)\n",
    "                            if not diff:\n",
    "                                continue\n",
    "                                \n",
    "                            # Find blame for changed lines\n",
    "                            blame_output = git_repo.git.blame(\n",
    "                                '-w', '-l', '-p',  # -w ignores whitespace, -l shows long hashes\n",
    "                                f\"{commit.hash}^\",  # Look at parent commit\n",
    "                                '--', modified_file.new_path\n",
    "                            )\n",
    "                            \n",
    "                            buggy_commits = parse_blame_for_lines(blame_output, diff)\n",
    "                            \n",
    "                            for buggy_hash in buggy_commits:\n",
    "                                try:\n",
    "                                    buggy_commit = git_repo.commit(buggy_hash)\n",
    "                                    # Prefer old_path for fault-inducing context, fallback to new_path\n",
    "                                    buggy_file_path = modified_file.old_path or modified_file.new_path\n",
    "\n",
    "                                    buggy_files.add(buggy_file_path)\n",
    "                                    writer.writerow({\n",
    "                                        'buggy_file_path': buggy_file_path,\n",
    "                                        'fix_commit_hash': commit.hash,\n",
    "                                        'fix_date': commit.committer_date,\n",
    "                                        'buggy_commit_hash': buggy_hash,\n",
    "                                        'buggy_date': buggy_commit.committed_datetime,\n",
    "                                        'fix_message': commit.msg[:200].replace('\\n', ' ')\n",
    "                                    })\n",
    "\n",
    "                                    print(f\"    Found buggy commit: {buggy_hash[:8]} for {modified_file.new_path}\")\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    print(f\"    Error processing buggy commit: {str(e)}\")\n",
    "                                    continue\n",
    "                                    \n",
    "                        except GitCommandError as e:\n",
    "                            print(f\"    Error processing {modified_file.new_path}: {str(e)}\")\n",
    "                            continue\n",
    "            \n",
    "            print(f\"\\nSummary for {start_release} to {end_release}:\")\n",
    "            print(f\"  Fix commits analyzed: {fix_count}\")\n",
    "            print(f\"  Buggy files found: {len(buggy_files)}\")\n",
    "            results[(start_release, end_release)] = buggy_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {start_release}-{end_release}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_changed_lines(old_content, new_content):\n",
    "    \"\"\"Identify changed lines between two file versions\"\"\"\n",
    "    old_lines = old_content.splitlines()\n",
    "    new_lines = new_content.splitlines()\n",
    "    diff = []\n",
    "    \n",
    "    for i, (old_line, new_line) in enumerate(zip(old_lines, new_lines)):\n",
    "        if old_line != new_line:\n",
    "            diff.append(i+1)  # Line numbers start at 1\n",
    "    \n",
    "    # Handle added/removed lines at the end\n",
    "    len_diff = len(new_lines) - len(old_lines)\n",
    "    if len_diff > 0:\n",
    "        diff.extend(range(len(old_lines)+1, len(new_lines)+1))\n",
    "    \n",
    "    return diff\n",
    "\n",
    "def parse_blame_for_lines(blame_output, target_lines):\n",
    "    \"\"\"Parse blame output for specific line numbers.\"\"\"\n",
    "    commits = set()\n",
    "    current_line = 0\n",
    "    commit_hash = None  # Ensure this is always defined\n",
    "\n",
    "    for line in blame_output.split('\\n'):\n",
    "        if len(line) >= 40 and re.match(r'^[0-9a-f]{40}', line):  # New commit hash line\n",
    "            commit_hash = line.split()[0]\n",
    "        elif line.startswith('filename '):\n",
    "            current_line += 1\n",
    "        elif line.startswith('\\t'):\n",
    "            current_line += 1\n",
    "            if current_line in target_lines and commit_hash:\n",
    "                commits.add(commit_hash)\n",
    "\n",
    "    return commits\n",
    "\n",
    "\n",
    "def is_fix_commit(commit_message):\n",
    "    \"\"\"Enhanced fix commit detection\"\"\"\n",
    "    error_keywords = [\n",
    "        \"fix\", \"crash\", \"resolves\", \"regression\", \"fall back\", \"assertion\", \"coverity\",\n",
    "    \"reproducible\", \"stack-wanted\", \"steps-wanted\", \"testcase\", \"fail\", \"npe\", \"except\",\n",
    "    \"broken\", \"bug\", \"differential testing\", \"error\", \"address sanitizer\", \"hang\",\n",
    "    \"perma orange\", \"random orange\", \"intermittent\", \"steps to reproduce\", \"leak\",\n",
    "    \"stack trace\", \"heap overflow\", \"freeze\", \"problem\", \"overflow\", \"avoid\", \"issue\",\n",
    "    \"workaround\", \"break\", \"stop\"\n",
    "    ]\n",
    "    \n",
    "    lower_msg = commit_message.lower()\n",
    "    \n",
    "    # More sophisticated detection\n",
    "    has_bug_number = any(word.isdigit() for word in lower_msg.split())\n",
    "    has_issue_ref = '#' in lower_msg or 'issue' in lower_msg\n",
    "    \n",
    "    return any(kw in lower_msg for kw in error_keywords) or has_bug_number or has_issue_ref\n",
    "# Define the release pairs you want to analyze\n",
    "release_pairs_to_analyze = [\n",
    "#   javacpp  (\"0.5\", \"0.9\"),\n",
    "#         (\"0.9\", \"1.1\"),\n",
    "#         (\"1.1\", \"1.2\"),\n",
    "#         (\"1.2\", \"1.2.1\"),\n",
    "#         (\"1.2.1\", \"1.2.7\"),\n",
    "#         (\"1.2.7\", \"1.3\"),\n",
    "#         (\"1.3\", \"1.3.2\"),\n",
    "#         (\"1.3.2\", \"1.4\"),\n",
    "#         (\"1.4\", \"1.4.2\"),\n",
    "#         (\"1.4.2\", \"1.4.4\"),\n",
    "#         (\"1.4.4\", \"1.5\"),\n",
    "#         (\"1.5\", \"1.5.1-1\"),\n",
    "#         (\"1.5.1-1\", \"1.5.2\"),\n",
    "#       rocksdb (\"v5.0.2\", \"v5.4.6\"),\n",
    "#         (\"v5.4.6\", \"v5.6.2\"),\n",
    "#         (\"v5.6.2\", \"v5.9.2\"),\n",
    "#         (\"v5.9.2\", \"v5.11.2\"),\n",
    "#         (\"v5.11.2\", \"v5.14.3\"),\n",
    "#         (\"v5.14.3\", \"v5.17.2\"),\n",
    "#         (\"v5.17.2\", \"v5.18.3\"),\n",
    "#         (\"v5.18.3\", \"v6.1.1\"),\n",
    "#         (\"v6.1.1\", \"v6.2.2\"),\n",
    "#         (\"v6.2.2\", \"v6.2.4\")\n",
    "#    jpype   (\"v0.5.4.5\", \"v0.5.5.1\"),\n",
    "#         (\"v0.5.5.1\", \"v0.5.5.4\"),\n",
    "#         (\"v0.5.5.4\", \"v0.5.6\"),\n",
    "#         (\"v0.5.6\", \"v0.5.7\"),\n",
    "#         (\"v0.5.7\", \"v0.6.0\"),\n",
    "#         (\"v0.6.0\", \"v0.6.1\"),\n",
    "#         (\"v0.6.1\", \"v0.6.2\"),\n",
    "#         (\"v0.6.2\", \"v0.6.3\"),\n",
    "#         (\"v0.6.3\", \"v0.7\"),\n",
    "#         (\"v0.7\", \"v0.7.1\"),\n",
    "#         (\"v0.7.1\", \"v0.7.2\")\n",
    "#      realm-java  (\"v0.90.0\", \"v1.2.0\"),\n",
    "#        (\"v1.2.0\", \"v2.3.2\"),\n",
    "#         (\"v2.3.2\", \"v3.7.2\"),\n",
    "#         (\"v3.7.2\", \"v4.4.0\"),\n",
    "#         (\"v4.4.0\", \"v5.4.0\"),\n",
    "#         (\"v5.4.0\", \"v5.7.1\"),\n",
    "#         (\"v5.7.1\", \"v5.9.0\"),\n",
    "#         (\"v5.9.0\", \"v5.11.0\"),\n",
    "#         (\"v5.11.0\", \"v5.15.0\"),\n",
    "#         (\"v5.15.0\", \"v6.0.0\"),\n",
    "#      zstd-jni  (\"v0.4.4\", \"v1.3.0-1\"),\n",
    "#        (\"v1.3.0-1\", \"v1.3.2-2\"),\n",
    "#         (\"v1.3.2-2\", \"v1.3.3-1\"),\n",
    "#         (\"v1.3.3-1\", \"v1.3.4-1\"),\n",
    "#         (\"v1.3.4-1\", \"v1.3.4-8\"),\n",
    "#         (\"v1.3.4-8\", \"v1.3.5-3\"),\n",
    "#         (\"v1.3.5-3\", \"v1.3.7-1\"),\n",
    "#         (\"v1.3.7-1\", \"v1.3.8-1\"),\n",
    "#         (\"v1.3.8-1\", \"v1.4.0-1\"),\n",
    "#         (\"v1.4.0-1\", \"v1.4.2-1\"),\n",
    "#         (\"v1.4.2-1\", \"v1.4.4-3\")\n",
    "#     conscrypt  (\"1.0.0.RC2\", \"1.0.0.RC8\"),\n",
    "#         (\"1.0.0.RC8\", \"1.0.0.RC11\"),\n",
    "#         (\"1.0.0.RC11\", \"1.0.0.RC14\"),\n",
    "#         (\"1.0.0.RC14\", \"1.0.1\"),\n",
    "#         (\"1.0.1\", \"1.0.2\"),\n",
    "#         (\"1.0.2\", \"1.1.1\"),\n",
    "#         (\"1.1.1\", \"1.2.0\"),\n",
    "#         (\"1.2.0\", \"1.4.2\"),\n",
    "#         (\"1.4.2\", \"2.1.0\"),\n",
    "#         (\"2.1.0\", \"2.2.1\")\n",
    "#  java-smt   (\"0.1\", \"0.3\"),\n",
    "#     (\"0.3\", \"0.5\"),\n",
    "  #   (\"0.5\", \"0.60\"),\n",
    "#     (\"0.60\", \"1.0.1\"),\n",
    "#         (\"1.0.1\", \"2.0.0\"),\n",
    "#         (\"2.0.0\", \"2.0.0-alpha\"),\n",
    "#     (\"2.0.0-alpha\", \"2.2.0\"),\n",
    "     #    (\"2.2.0\", \"3.0.0\"),\n",
    "#     (\"3.0.0\", \"3.1.0\"),\n",
    "#         (\"3.1.0\", \"3.3.0\")\n",
    "# vlc  (\"2.5.4\", \"3.0.0\"),\n",
    "#     (\"3.0.0\", \"3.0.11\"),\n",
    "#         (\"3.0.11\", \"3.0.13\"),\n",
    "#         (\"3.0.13\", \"3.0.92\"),\n",
    "#         (\"3.0.92\", \"3.0.96\"),\n",
    "#         (\"3.0.96\", \"3.1.0\"),\n",
    "#         (\"3.1.0\", \"3.1.2\"),\n",
    "#         (\"3.1.2\", \"3.1.6\"),\n",
    "#         (\"3.1.6\", \"3.1.7\"),\n",
    "#         (\"3.1.7\", \"3.2.2\")\n",
    "# pljava    (\"V1_2_0\", \"V1_3_0\"),\n",
    "#     (\"V1_3_0\", \"V1_4_0\"),\n",
    "#     (\"V1_4_0\", \"V1_4_2\"),\n",
    "#     (\"V1_4_2\", \"V1_4_3\"),\n",
    "#   (\"V1_4_3\", \"REL1_5_STABLE-BASE\"),\n",
    "#         (\"REL1_5_STABLE-BASE\", \"V1_5_0b3\"),\n",
    "#         (\"V1_5_0b3\", \"V1_5_0\"),\n",
    "#         (\"V1_5_0\", \"V1_5_1b1\"),\n",
    "#         (\"V1_5_1b1\", \"V1_5_1b2\"),\n",
    "#         (\"V1_5_1b2\", \"V1_5_2\"),\n",
    "#         (\"V1_5_2\", \"V1_5_3\"),\n",
    "#         (\"V1_5_3\", \"V1_5_5\")\n",
    "   #sqlite\n",
    "    (\"3.42.0.1\", \"3.44.0.0\"),\n",
    "    (\"3.44.0.0\", \"3.45.0.0\"),\n",
    "    (\"3.45.0.0\", \"3.45.2.0\"),\n",
    "    (\"3.45.2.0\", \"3.46.0.0\"),\n",
    "    (\"3.46.0.0\", \"3.46.1.1\"),\n",
    "    (\"3.46.1.1\", \"3.47.0.0\"),\n",
    "    (\"3.47.0.0\", \"3.47.2.0\"),\n",
    "    (\"3.47.2.0\", \"3.49.0.0\"),\n",
    "    (\"3.49.0.0\", \"3.50.1.0\"),\n",
    "    (\"3.50.1.0\", \"3.50.3.0\")\n",
    "    #jni-bind\n",
    "#     (\"Release-0.8.0-alpha\",\"Release-0.9.1-alpha\"),\n",
    "#     (\"Release-0.9.1-alpha\",\"Release-0.9.3-alpha\"),\n",
    "#     (\"Release-0.9.3-alpha\",\"Release-0.9.6-alpha\"),\n",
    "#     (\"Release-0.9.6-alpha\",\"Release-0.9.7-alpha\"),\n",
    "#     (\"Release-0.9.7-alpha\",\"Release-0.9.8-alpha\"),\n",
    "#     (\"Release-0.9.8-alpha\",\"Release-0.9.9-alpha\"),\n",
    "#     (\"Release-0.9.9-alpha\",\"Release-1.0.0-beta\"),\n",
    "#     (\"Release-1.0.0-beta\",\"Release-1.1.0-beta\"),\n",
    "#     (\"Release-1.1.0-beta\",\"Release-1.1.2-beta\"),\n",
    "#     (\"Release-1.1.2-beta\",\"Release-1.2.3\")\n",
    "    #Monero-java\n",
    "    \n",
    "#     (\"v0.8.9\",\"v0.8.10\"),\n",
    "#     (\"v0.8.10\",\"v0.8.13\"),\n",
    "#     (\"v0.8.13\",\"v0.8.17\"),\n",
    "#     (\"v0.8.17\",\"v0.8.24\"),\n",
    "#     (\"v0.8.24\",\"v0.8.31\"),\n",
    "#     (\"v0.8.31\",\"v0.8.35\"),\n",
    "#     (\"v0.8.35\",\"v0.8.36\"),\n",
    "#     (\"v0.8.36\",\"v0.8.37\"),\n",
    "#     (\"v0.8.37\",\"v0.8.38\"),\n",
    "#     (\"v0.8.38\",\"v0.8.39\")\n",
    "    #webrtc\n",
    "#     (\"v0.2.0\", \"v0.3.0\"),\n",
    "#     (\"v0.3.0\", \"v0.4.0\"),\n",
    "#     (\"v0.4.0\", \"v0.6.0\"),\n",
    "#     (\"v0.6.0\", \"v0.7.0\"),\n",
    "#     (\"v0.7.0\", \"v0.8.0\"),\n",
    "#     (\"v0.8.0\", \"v0.10.0\"),\n",
    "#     (\"v0.10.0\", \"v0.11.0\"),\n",
    "#     (\"v0.11.0\", \"v0.12.0\"),\n",
    "#     (\"v0.12.0\", \"v0.13.0\"),\n",
    "#     (\"v0.13.0\", \"v0.14.0\")\n",
    "    #wolfcrypt\n",
    "#     (\"v1.0.0-stable\", \"v1.1.0-stable\"),\n",
    "#     (\"v1.1.0-stable\", \"v1.2.0-stable\"),\n",
    "#     (\"v1.2.0-stable\", \"v1.3.0-stable\"),\n",
    "#     (\"v1.3.0-stable\", \"v1.5.0-stable\"),\n",
    "#     (\"v1.5.0-stable\", \"v1.6.0-stable\"),\n",
    "#     (\"v1.6.0-stable\", \"v1.7.0-stable\"),\n",
    "#     (\"v1.7.0-stable\", \"v1.8.0-stable\")\n",
    "    #wolfssl\n",
    "#     (\"v1.4.0-stable\", \"v1.5.0-stable\"),\n",
    "#     (\"v1.5.0-stable\", \"v1.6.0-stable\"),\n",
    "#     (\"v1.6.0-stable\", \"v1.8.0-stable\"),\n",
    "#     (\"v1.8.0-stable\", \"v1.9.0-stable\"),\n",
    "#     (\"v1.9.0-stable\", \"v1.11.0-stable\"),\n",
    "#     (\"v1.11.0-stable\", \"v1.12.0-stable\"),\n",
    "#     (\"v1.12.0-stable\", \"v1.12.2\"),\n",
    "#     (\"v1.12.2\", \"v1.13.0-stable\"),\n",
    "#     (\"v1.13.0-stable\", \"v1.14.0-stable\"),\n",
    "#     (\"v1.14.0-stable\", \"v1.15.0-stable\")\n",
    "    \n",
    "]\n",
    "\n",
    "# Run the analysis\n",
    "results = detect_fault_inducing_commits(\n",
    "    repo_path=\"revision projects/sqlite/3.50.3.0\",\n",
    "    release_pairs=release_pairs_to_analyze,\n",
    "    output_dir=\"buggy_smelly/revision/general_changed\"\n",
    ")\n",
    "\n",
    "# Access results programmatically if needed\n",
    "for release_pair, buggy_files in results.items():\n",
    "    print(f\"Between {release_pair[0]} and {release_pair[1]}, found {len(buggy_files)} buggy files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faulty files using pydriller and fault inducing commits\n",
    "import os\n",
    "import csv\n",
    "from pydriller import Repository\n",
    "from git import Repo, GitCommandError\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "def detect_fault_inducing_commits(repo_path, release_pairs, output_dir=\"buggy_files_reports\"):\n",
    "    \"\"\"\n",
    "    Enhanced fault-inducing commit detection with better blame handling and debugging.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    git_repo = Repo(repo_path)\n",
    "    results = {}\n",
    "    project_name = repo_path.split(\"/\")[-1]\n",
    "    if project_name==\"vlc-android\":\n",
    "        project_name=\"vlc\"\n",
    "\n",
    "    for start_release, end_release in release_pairs:\n",
    "        try:\n",
    "            print(f\"\\nAnalyzing {start_release} to {end_release}\")\n",
    "            \n",
    "            # Get commit range\n",
    "            start_commit = git_repo.tags[start_release].commit\n",
    "            end_commit = git_repo.tags[end_release].commit\n",
    "            \n",
    "            csv_filename = os.path.join(output_dir, f\"wolfssl-{start_release}.csv\")\n",
    "            buggy_files = set()\n",
    "            fix_count = 0\n",
    "            \n",
    "            with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "                    'buggy_file_path', 'fix_commit_hash', 'fix_date',\n",
    "                    'buggy_commit_hash', 'buggy_date', 'fix_message'\n",
    "                ])\n",
    "                writer.writeheader()\n",
    "                \n",
    "                # Traverse commits with PyDriller\n",
    "                for commit in Repository(\n",
    "                    repo_path,\n",
    "                    since=start_commit.committed_datetime,\n",
    "                    to=end_commit.committed_datetime,\n",
    "                    only_modifications_with_file_types=['.java', '.cpp', '.c', '.h']  # Filter by file type\n",
    "                ).traverse_commits():\n",
    "                    \n",
    "                    if not is_fix_commit(commit.msg):\n",
    "                        continue\n",
    "                    \n",
    "                    fix_count += 1\n",
    "                    print(f\"  Fix commit: {commit.hash[:8]} - {commit.msg[:50]}...\")\n",
    "                    \n",
    "                    for modified_file in commit.modified_files:\n",
    "                        if not modified_file.new_path:\n",
    "                            continue\n",
    "                            \n",
    "                        try:\n",
    "                            # Get previous version of the file\n",
    "                            old_path = modified_file.old_path or modified_file.new_path\n",
    "                            previous_contents = git_repo.git.show(f\"{commit.hash}^:{old_path}\")\n",
    "                            current_contents = git_repo.git.show(f\"{commit.hash}:{modified_file.new_path}\")\n",
    "                            \n",
    "                            # Get changed lines\n",
    "                            diff = get_changed_lines(previous_contents, current_contents)\n",
    "                            if not diff:\n",
    "                                continue\n",
    "                                \n",
    "                            # Find blame for changed lines\n",
    "                            blame_output = git_repo.git.blame(\n",
    "                                '-w', '-l', '-p',  # -w ignores whitespace, -l shows long hashes\n",
    "                                f\"{commit.hash}^\",  # Look at parent commit\n",
    "                                '--', modified_file.new_path\n",
    "                            )\n",
    "                            \n",
    "                            buggy_commits = parse_blame_for_lines(blame_output, diff)\n",
    "                            \n",
    "                            for buggy_hash in buggy_commits:\n",
    "                                try:\n",
    "                                    buggy_commit = git_repo.commit(buggy_hash)\n",
    "                                    # Prefer old_path for fault-inducing context, fallback to new_path\n",
    "                                    buggy_file_path = modified_file.old_path or modified_file.new_path\n",
    "\n",
    "                                    buggy_files.add(buggy_file_path)\n",
    "                                    writer.writerow({\n",
    "                                        'buggy_file_path': buggy_file_path,\n",
    "                                        'fix_commit_hash': commit.hash,\n",
    "                                        'fix_date': commit.committer_date,\n",
    "                                        'buggy_commit_hash': buggy_hash,\n",
    "                                        'buggy_date': buggy_commit.committed_datetime,\n",
    "                                        'fix_message': commit.msg[:200].replace('\\n', ' ')\n",
    "                                    })\n",
    "\n",
    "                                    print(f\"    Found buggy commit: {buggy_hash[:8]} for {modified_file.new_path}\")\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    print(f\"    Error processing buggy commit: {str(e)}\")\n",
    "                                    continue\n",
    "                                    \n",
    "                        except GitCommandError as e:\n",
    "                            print(f\"    Error processing {modified_file.new_path}: {str(e)}\")\n",
    "                            continue\n",
    "            \n",
    "            print(f\"\\nSummary for {start_release} to {end_release}:\")\n",
    "            print(f\"  Fix commits analyzed: {fix_count}\")\n",
    "            print(f\"  Buggy files found: {len(buggy_files)}\")\n",
    "            results[(start_release, end_release)] = buggy_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {start_release}-{end_release}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_changed_lines(old_content, new_content):\n",
    "    \"\"\"Identify changed lines between two file versions\"\"\"\n",
    "    old_lines = old_content.splitlines()\n",
    "    new_lines = new_content.splitlines()\n",
    "    diff = []\n",
    "    \n",
    "    for i, (old_line, new_line) in enumerate(zip(old_lines, new_lines)):\n",
    "        if old_line != new_line:\n",
    "            diff.append(i+1)  # Line numbers start at 1\n",
    "    \n",
    "    # Handle added/removed lines at the end\n",
    "    len_diff = len(new_lines) - len(old_lines)\n",
    "    if len_diff > 0:\n",
    "        diff.extend(range(len(old_lines)+1, len(new_lines)+1))\n",
    "    \n",
    "    return diff\n",
    "\n",
    "def parse_blame_for_lines(blame_output, target_lines):\n",
    "    \"\"\"Parse blame output for specific line numbers.\"\"\"\n",
    "    commits = set()\n",
    "    current_line = 0\n",
    "    commit_hash = None  # Ensure this is always defined\n",
    "\n",
    "    for line in blame_output.split('\\n'):\n",
    "        if len(line) >= 40 and re.match(r'^[0-9a-f]{40}', line):  # New commit hash line\n",
    "            commit_hash = line.split()[0]\n",
    "        elif line.startswith('filename '):\n",
    "            current_line += 1\n",
    "        elif line.startswith('\\t'):\n",
    "            current_line += 1\n",
    "            if current_line in target_lines and commit_hash:\n",
    "                commits.add(commit_hash)\n",
    "\n",
    "    return commits\n",
    "\n",
    "\n",
    "def is_fix_commit(commit_message):\n",
    "    \"\"\"Enhanced fix commit detection\"\"\"\n",
    "    error_keywords = [\n",
    "        \"fix\", \"crash\", \"resolves\", \"regression\", \"fall back\", \"assertion\", \"coverity\",\n",
    "    \"reproducible\", \"stack-wanted\", \"steps-wanted\", \"testcase\", \"fail\", \"npe\", \"except\",\n",
    "    \"broken\", \"bug\", \"differential testing\", \"error\", \"address sanitizer\", \"hang\",\n",
    "    \"perma orange\", \"random orange\", \"intermittent\", \"steps to reproduce\",\"assertion\", \"leak\",\n",
    "    \"stack trace\", \"heap overflow\", \"freez\",\"str\", \"problem\", \"overflow\", \"avoid\", \"issue\",\n",
    "    \"workaround\", \"break\", \"stop\"\n",
    "    ]\n",
    "    \n",
    "    lower_msg = commit_message.lower()\n",
    "    \n",
    "    # More sophisticated detection\n",
    "    has_bug_number = any(word.isdigit() for word in lower_msg.split())\n",
    "    has_issue_ref = '#' in lower_msg or 'issue' in lower_msg\n",
    "    \n",
    "    return any(kw in lower_msg for kw in error_keywords) or has_bug_number or has_issue_ref\n",
    "# Define the release pairs you want to analyze\n",
    "release_pairs_to_analyze = [\n",
    "#    javacpp (\"0.5\", \"0.9\"),\n",
    "#         (\"0.9\", \"1.1\"),\n",
    "#         (\"1.1\", \"1.2\"),\n",
    "#         (\"1.2\", \"1.2.1\"),\n",
    "#         (\"1.2.1\", \"1.2.7\"),\n",
    "#         (\"1.2.7\", \"1.3\"),\n",
    "#         (\"1.3\", \"1.3.2\"),\n",
    "#         (\"1.3.2\", \"1.4\"),\n",
    "#         (\"1.4\", \"1.4.2\"),\n",
    "#         (\"1.4.2\", \"1.4.4\"),\n",
    "#         (\"1.4.4\", \"1.5\"),\n",
    "#         (\"1.5\", \"1.5.1-1\"),\n",
    "#         (\"1.5.1-1\", \"1.5.2\"),\n",
    "\n",
    "    #     (\"v5.0.2\", \"v5.4.6\"),\n",
    "#         (\"v5.4.6\", \"v5.6.2\"),\n",
    "#         (\"v5.6.2\", \"v5.9.2\"),\n",
    "#         (\"v5.9.2\", \"v5.11.2\"),\n",
    "#         (\"v5.11.2\", \"v5.14.3\"),\n",
    "#         (\"v5.14.3\", \"v5.17.2\"),\n",
    "#         (\"v5.17.2\", \"v5.18.3\"),\n",
    "#         (\"v5.18.3\", \"v6.1.1\"),\n",
    "#         (\"v6.1.1\", \"v6.2.2\"),\n",
    "#         (\"v6.2.2\", \"v6.2.4\")\n",
    "#     jpype (\"v0.5.4.5\", \"v0.5.5.1\"),\n",
    "#         (\"v0.5.5.1\", \"v0.5.5.4\"),\n",
    "#         (\"v0.5.5.4\", \"v0.5.6\"),\n",
    "#         (\"v0.5.6\", \"v0.5.7\"),\n",
    "#         (\"v0.5.7\", \"v0.6.0\"),\n",
    "#         (\"v0.6.0\", \"v0.6.1\"),\n",
    "#         (\"v0.6.1\", \"v0.6.2\"),\n",
    "#         (\"v0.6.2\", \"v0.6.3\"),\n",
    "#         (\"v0.6.3\", \"v0.7\"),\n",
    "      #  (\"v0.7\", \"v0.7.1\")\n",
    "   # (\"v0.7.1\", \"v0.7.2\")\n",
    "  \n",
    "#     realm-java   (\"v0.90.0\", \"v1.2.0\"),\n",
    "#        (\"v1.2.0\", \"v2.3.2\"),\n",
    "#         (\"v2.3.2\", \"v3.7.2\"),\n",
    "#         (\"v3.7.2\", \"v4.4.0\"),\n",
    "#         (\"v4.4.0\", \"v5.4.0\"),\n",
    "#         (\"v5.4.0\", \"v5.7.1\"),\n",
    "#         (\"v5.7.1\", \"v5.9.0\"),\n",
    "#         (\"v5.9.0\", \"v5.11.0\"),\n",
    "#         (\"v5.11.0\", \"v5.15.0\"),\n",
    "#         (\"v5.15.0\", \"v6.0.0\"),\n",
    "#     zstd-jni   (\"v0.4.4\", \"v1.3.0-1\"),\n",
    "#        (\"v1.3.0-1\", \"v1.3.2-2\"),\n",
    "#         (\"v1.3.2-2\", \"v1.3.3-1\"),\n",
    "#         (\"v1.3.3-1\", \"v1.3.4-1\"),\n",
    "#         (\"v1.3.4-1\", \"v1.3.4-8\"),\n",
    "#         (\"v1.3.4-8\", \"v1.3.5-3\"),\n",
    "#         (\"v1.3.5-3\", \"v1.3.7-1\"),\n",
    "#         (\"v1.3.7-1\", \"v1.3.8-1\"),\n",
    "#         (\"v1.3.8-1\", \"v1.4.0-1\"),\n",
    "#         (\"v1.4.0-1\", \"v1.4.2-1\"),\n",
    "#         (\"v1.4.2-1\", \"v1.4.4-3\")\n",
    "#      conscrypt (\"1.0.0.RC2\", \"1.0.0.RC8\"),\n",
    "#         (\"1.0.0.RC8\", \"1.0.0.RC11\"),\n",
    "#         (\"1.0.0.RC11\", \"1.0.0.RC14\"),\n",
    "#         (\"1.0.0.RC14\", \"1.0.1\"),\n",
    "#         (\"1.0.1\", \"1.0.2\"),\n",
    "#         (\"1.0.2\", \"1.1.1\"),\n",
    "#         (\"1.1.1\", \"1.2.0\"),\n",
    "#         (\"1.2.0\", \"1.4.2\"),\n",
    "#         (\"1.4.2\", \"2.1.0\"),\n",
    "#         (\"2.1.0\", \"2.2.1\")\n",
    "# java-smt (\"0.1\", \"0.3\"),\n",
    "#     (\"0.3\", \"0.5\"),\n",
    " #   (\"0.5\", \"0.60\"),\n",
    "#     (\"0.60\", \"1.0.1\"),\n",
    "#         (\"1.0.1\", \"2.0.0\"),\n",
    "#         (\"2.0.0\", \"2.0.0-alpha\"),\n",
    "#     (\"2.0.0-alpha\", \"2.2.0\"),\n",
    "  #      (\"2.2.0\", \"3.0.0\"),\n",
    "#     (\"3.0.0\", \"3.1.0\"),\n",
    "#         (\"3.1.0\", \"3.3.0\")\n",
    "# vlv (\"2.5.4\", \"3.0.0\"),\n",
    "#     (\"3.0.0\", \"3.0.11\"),\n",
    "#         (\"3.0.11\", \"3.0.13\"),\n",
    "#         (\"3.0.13\", \"3.0.92\"),\n",
    "#         (\"3.0.92\", \"3.0.96\"),\n",
    "#         (\"3.0.96\", \"3.1.0\"),\n",
    "#         (\"3.1.0\", \"3.1.2\"),\n",
    "#         (\"3.1.2\", \"3.1.6\"),\n",
    "#         (\"3.1.6\", \"3.1.7\"),\n",
    "#         (\"3.1.7\", \"3.2.2\")\n",
    "# pljava  (\"V1_2_0\", \"V1_3_0\"),\n",
    "#     (\"V1_3_0\", \"V1_4_0\"),\n",
    "#     (\"V1_4_0\", \"V1_4_2\"),\n",
    "#     (\"V1_4_2\", \"V1_4_3\"),\n",
    "#   (\"V1_4_3\", \"REL1_5_STABLE-BASE\"),\n",
    "#         (\"REL1_5_STABLE-BASE\", \"V1_5_0b3\"),\n",
    "#         (\"V1_5_0b3\", \"V1_5_0\"),\n",
    "#         (\"V1_5_0\", \"V1_5_1b1\"),\n",
    "#         (\"V1_5_1b1\", \"V1_5_1b2\"),\n",
    "#         (\"V1_5_1b2\", \"V1_5_2\"),\n",
    "#         (\"V1_5_2\", \"V1_5_3\"),\n",
    "#         (\"V1_5_3\", \"V1_5_5\")\n",
    "    #sqlite\n",
    "#     (\"3.42.0.1\", \"3.44.0.0\"),\n",
    "#     (\"3.44.0.0\", \"3.45.0.0\"),\n",
    "#     (\"3.45.0.0\", \"3.45.2.0\"),\n",
    "#     (\"3.45.2.0\", \"3.46.0.0\"),\n",
    "#     (\"3.46.0.0\", \"3.46.1.1\"),\n",
    "#     (\"3.46.1.1\", \"3.47.0.0\"),\n",
    "#     (\"3.47.0.0\", \"3.47.2.0\"),\n",
    "#     (\"3.47.2.0\", \"3.49.0.0\"),\n",
    "#     (\"3.49.0.0\", \"3.50.1.0\"),\n",
    "#     (\"3.50.1.0\", \"3.50.3.0\")\n",
    "    #jni-bind\n",
    "#     (\"Release-0.8.0-alpha\",\"Release-0.9.1-alpha\"),\n",
    "#     (\"Release-0.9.1-alpha\",\"Release-0.9.3-alpha\"),\n",
    "#     (\"Release-0.9.3-alpha\",\"Release-0.9.6-alpha\"),\n",
    "#     (\"Release-0.9.6-alpha\",\"Release-0.9.7-alpha\"),\n",
    "#     (\"Release-0.9.7-alpha\",\"Release-0.9.8-alpha\"),\n",
    "#     (\"Release-0.9.8-alpha\",\"Release-0.9.9-alpha\"),\n",
    "#     (\"Release-0.9.9-alpha\",\"Release-1.0.0-beta\"),\n",
    "#     (\"Release-1.0.0-beta\",\"Release-1.1.0-beta\"),\n",
    "#     (\"Release-1.1.0-beta\",\"Release-1.1.2-beta\"),\n",
    "#     (\"Release-1.1.2-beta\",\"Release-1.2.3\")\n",
    "    #Monero-java\n",
    "    \n",
    "#     (\"v0.8.9\",\"v0.8.10\"),\n",
    "#     (\"v0.8.10\",\"v0.8.13\"),\n",
    "#     (\"v0.8.13\",\"v0.8.17\"),\n",
    "#     (\"v0.8.17\",\"v0.8.24\"),\n",
    "#     (\"v0.8.24\",\"v0.8.31\"),\n",
    "#     (\"v0.8.31\",\"v0.8.35\"),\n",
    "#     (\"v0.8.35\",\"v0.8.36\"),\n",
    "#     (\"v0.8.36\",\"v0.8.37\"),\n",
    "#     (\"v0.8.37\",\"v0.8.38\"),\n",
    "#     (\"v0.8.38\",\"v0.8.39\")\n",
    "    #webrtc\n",
    "#     (\"v0.2.0\", \"v0.3.0\"),\n",
    "#     (\"v0.3.0\", \"v0.4.0\"),\n",
    "#     (\"v0.4.0\", \"v0.6.0\"),\n",
    "#     (\"v0.6.0\", \"v0.7.0\"),\n",
    "#     (\"v0.7.0\", \"v0.8.0\"),\n",
    "#     (\"v0.8.0\", \"v0.10.0\"),\n",
    "#     (\"v0.10.0\", \"v0.11.0\"),\n",
    "#     (\"v0.11.0\", \"v0.12.0\"),\n",
    "#     (\"v0.12.0\", \"v0.13.0\"),\n",
    "#     (\"v0.13.0\", \"v0.14.0\")\n",
    "    #wolfcrypt\n",
    "#     (\"v1.0.0-stable\", \"v1.1.0-stable\"),\n",
    "#     (\"v1.1.0-stable\", \"v1.2.0-stable\"),\n",
    "#     (\"v1.2.0-stable\", \"v1.3.0-stable\"),\n",
    "#     (\"v1.3.0-stable\", \"v1.5.0-stable\"),\n",
    "#     (\"v1.5.0-stable\", \"v1.6.0-stable\"),\n",
    "#     (\"v1.6.0-stable\", \"v1.7.0-stable\"),\n",
    "#     (\"v1.7.0-stable\", \"v1.8.0-stable\")\n",
    "    #wolfssl\n",
    "    (\"v1.4.0-stable\", \"v1.5.0-stable\"),\n",
    "    (\"v1.5.0-stable\", \"v1.6.0-stable\"),\n",
    "    (\"v1.6.0-stable\", \"v1.8.0-stable\"),\n",
    "    (\"v1.8.0-stable\", \"v1.9.0-stable\"),\n",
    "    (\"v1.9.0-stable\", \"v1.11.0-stable\"),\n",
    "    (\"v1.11.0-stable\", \"v1.12.0-stable\"),\n",
    "    (\"v1.12.0-stable\", \"v1.12.2\"),\n",
    "    (\"v1.12.2\", \"v1.13.0-stable\"),\n",
    "    (\"v1.13.0-stable\", \"v1.14.0-stable\"),\n",
    "    (\"v1.14.0-stable\", \"v1.15.0-stable\"),\n",
    "    \n",
    "]\n",
    "\n",
    "# Run the analysis\n",
    "results = detect_fault_inducing_commits(\n",
    "    repo_path=\"revision projects/wolfssl/v1.15.0-stable\",\n",
    "    release_pairs=release_pairs_to_analyze,\n",
    "    output_dir=\"buggy_smelly/revision/general_faulty\"\n",
    ")\n",
    "\n",
    "# Access results programmatically if needed\n",
    "for release_pair, buggy_files in results.items():\n",
    "    print(f\"Between {release_pair[0]} and {release_pair[1]}, found {len(buggy_files)} buggy files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dab599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out jni files from the general changed files and faulty files\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Modifying the jni file names because it contains the repo name and project name also which is not there in smelly and faulty files\n",
    "def transform_jni_path(path, project_name):\n",
    "    path = str(path).strip()\n",
    "    path = path.replace(\"revision projects/extra projects/\", \"\")\n",
    "    path = path.replace(\"revision projects/\", \"\")\n",
    "    path = path.replace(f\"{project_name}/\",\"\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# Directories\n",
    "#changed_dir = \"buggy_smelly/abidi/general_changed_faulty/changed_files\"\n",
    "changed_dir = \"buggy_smelly/revision/general_faulty\"\n",
    "#buggy_dir = \"buggy_smelly/abidi/general_changed_faulty/fault_fixing_changed_files\"\n",
    "jni_dir = \"buggy_smelly/revision/JNIfiles\"  # <-- JNI files by project\n",
    "output_dir = \"buggy_smelly/revision/FaultyJNI\"\n",
    "\n",
    "# Get sorted file lists\n",
    "changed_files = sorted(os.listdir(changed_dir))\n",
    "#buggy_files = sorted(os.listdir(buggy_dir))\n",
    "jni_files = sorted(os.listdir(jni_dir))\n",
    "\n",
    "print(len(changed_files))\n",
    "print(len(jni_files))\n",
    "# Ensure the same number of files exist\n",
    "#assert len(changed_files) == len(jni_files), \"Mismatch in number of files across directories!\"\n",
    "\n",
    "for changed_file, jni_file in zip(changed_files, jni_files):\n",
    "        project_name = changed_file.replace(\".csv\", \"\")\n",
    "\n",
    "        changed_path = os.path.join(changed_dir, changed_file)\n",
    "        #buggy_path = os.path.join(buggy_dir, buggy_file)\n",
    "        jni_path = os.path.join(jni_dir, changed_file)\n",
    "\n",
    "        # Read CSVs\n",
    "        changed_df = pd.read_csv(changed_path)\n",
    "        #buggy_df = pd.read_csv(buggy_path)\n",
    "        jni_df = pd.read_csv(jni_path)\n",
    "\n",
    "        # Get sets of file names (first column assumed)\n",
    "        changed_set = set(changed_df.iloc[:, 0].dropna())\n",
    "        #buggy_set = set(buggy_df.iloc[:, 0].dropna())\n",
    "        jni_set = set(transform_jni_path(p, project_name) for p in jni_df.iloc[:, 0].dropna())\n",
    "\n",
    "\n",
    "        # Filter only those in the JNI list\n",
    "        changed_jni = changed_set & jni_set\n",
    "        #buggy_jni = buggy_set & jni_set\n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "        with open(os.path.join(output_dir, f\"{project_name}.csv\"), 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"File Path\"])\n",
    "            for fpath in sorted(changed_jni):\n",
    "                writer.writerow([fpath])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ae305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to compute, smelly, change/buggy, and smelly_buggy files \n",
    "# this checks whether the smelly filename contains the jni file name because smelly filename also contains system folder names and project and release names\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Normalization used for JNI / smelly / buggy paths\n",
    "def transform_jni_path(path, project_name):\n",
    "    path = str(path).strip()\n",
    "    if not path:\n",
    "        return \"\"\n",
    "    # unify separators\n",
    "    path = path.replace(\"\\\\\", \"/\")\n",
    "    # remove common prefixes (keep / adjust as needed)\n",
    "    path = path.replace(\"/home/shahrukh/smellDetection/Detection/revision projects/extra projects/\", \"\")\n",
    "    path = path.replace(\"/home/shahrukh/smellDetection/Detection/cloned_abidi/\", \"\")\n",
    "    path = path.replace(\"/home/abhaya/shahrukh/cloned_abidi/remainingForEILC/\", \"\")\n",
    "    path = path.replace(\"/home/abhaya/shahrukh/cloned_abidi/\", \"\")\n",
    "    path = path.replace(\"cloned_abidi/\", \"\")\n",
    "    # remove project name if present\n",
    "    path = path.replace(f\"{project_name}/\", \"\")\n",
    "    # trim leading/trailing ./ and slashes\n",
    "    path = path.lstrip(\"./\").strip(\"/\")\n",
    "    return path\n",
    "\n",
    "# Directories\n",
    "smelly_dir = \"buggy_smelly/revision/generalSmelly\"\n",
    "buggy_dir = \"buggy_smelly/revision/ChangedJNI\"\n",
    "jni_dir = \"buggy_smelly/revision/JNIfiles\"  # <-- JNI files by project\n",
    "output_csv = \"buggy_smelly/revision/fisher/data_for_change_analysis.csv\"\n",
    "\n",
    "# Get sorted file lists (only csv files)\n",
    "smelly_files = sorted([f for f in os.listdir(smelly_dir) if f.endswith(\".csv\")])\n",
    "buggy_files = sorted([f for f in os.listdir(buggy_dir) if f.endswith(\".csv\")])\n",
    "jni_files = sorted([f for f in os.listdir(jni_dir) if f.endswith(\".csv\")])\n",
    "\n",
    "# Make sure we iterate consistently: we'll zip the three lists.\n",
    "# If some projects are missing in one dir, consider aligning by project names instead.\n",
    "with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\n",
    "        \"Project Name\", \"JNI Files (count)\", \"Smelly JNI (count)\", \"Buggy JNI (count)\",\n",
    "        \"Smelly But Not Buggy\", \"Buggy But Not Smelly\", \"Both Smelly & Buggy\", \"Neither\"\n",
    "    ])\n",
    "\n",
    "    for smelly_file, buggy_file, jni_file in zip(smelly_files, buggy_files, jni_files):\n",
    "        project_name = smelly_file.replace(\".csv\", \"\")\n",
    "\n",
    "        smelly_path = os.path.join(smelly_dir, smelly_file)\n",
    "        buggy_path = os.path.join(buggy_dir, buggy_file)   # fixed: use buggy_file\n",
    "        jni_path = os.path.join(jni_dir, jni_file)         # fixed: use jni_file\n",
    "\n",
    "        # Read CSVs safely (if file is empty or has issues, continue)\n",
    "        try:\n",
    "            smelly_df = pd.read_csv(smelly_path, header=None, dtype=str)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: couldn't read {smelly_path}: {e}\")\n",
    "            smelly_df = pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            buggy_df = pd.read_csv(buggy_path, header=None, dtype=str)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: couldn't read {buggy_path}: {e}\")\n",
    "            buggy_df = pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            jni_df = pd.read_csv(jni_path, header=None, dtype=str)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: couldn't read {jni_path}: {e}\")\n",
    "            jni_df = pd.DataFrame()\n",
    "\n",
    "        # Extract lists (first column assumed). If the CSV is empty, produce empty list.\n",
    "        smelly_raw = smelly_df.iloc[:, 0].dropna().tolist() if smelly_df.shape[1] >= 1 else []\n",
    "        buggy_raw  = buggy_df.iloc[:, 0].dropna().tolist()  if buggy_df.shape[1] >= 1 else []\n",
    "        jni_raw    = jni_df.iloc[:, 0].dropna().tolist()    if jni_df.shape[1] >= 1 else []\n",
    "\n",
    "        # Normalize all paths (use same normalizer so containment checks are consistent)\n",
    "        smelly_paths = [transform_jni_path(p, project_name) for p in smelly_raw]\n",
    "        buggy_paths  = [transform_jni_path(p, project_name) for p in buggy_raw]\n",
    "        jni_paths    = [transform_jni_path(p, project_name) for p in jni_raw]\n",
    "\n",
    "        # Unique JNI set\n",
    "        jni_set = set(p for p in jni_paths if p)\n",
    "\n",
    "        # Determine which JNI files are smelly: if any smelly_path contains the jni filename\n",
    "        smelly_jni = set()\n",
    "        for jni in jni_set:\n",
    "            # containment check: jni contained in any smelly path\n",
    "            if any(jni in sp for sp in smelly_paths):\n",
    "                smelly_jni.add(jni)\n",
    "\n",
    "        # Determine which JNI files are buggy: if any buggy_path contains the jni filename\n",
    "        buggy_jni = set()\n",
    "        for jni in jni_set:\n",
    "            if any(jni in bp for bp in buggy_paths):\n",
    "                buggy_jni.add(jni)\n",
    "\n",
    "        # Compute metrics\n",
    "        jni_count = len(jni_set)\n",
    "        smelly_count = len(smelly_jni)\n",
    "        buggy_count = len(buggy_jni)\n",
    "        smelly_not_buggy = len(smelly_jni - buggy_jni)\n",
    "        buggy_not_smelly = len(buggy_jni - smelly_jni)\n",
    "        both = len(smelly_jni & buggy_jni)\n",
    "        neither = jni_count - (smelly_not_buggy + buggy_not_smelly + both)\n",
    "\n",
    "        print(f\"Processed {project_name}: JNI={jni_count}, Smelly={smelly_count}, Buggy={buggy_count}, \"\n",
    "              f\"Smelly_Not_Buggy={smelly_not_buggy}, Buggy_Not_Smelly={buggy_not_smelly}, Both={both}, Neither={neither}\")\n",
    "\n",
    "        writer.writerow([\n",
    "            project_name, jni_count, smelly_count, buggy_count,\n",
    "            smelly_not_buggy, buggy_not_smelly, both, neither\n",
    "        ])\n",
    "\n",
    "print(\"âœ… Processing complete. Results saved to:\", output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8273f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fisher exact test\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "def fisher_test_with_adjustment(a, b, c, d, confidence_level=0.95, alternative='greater'):\n",
    "    \"\"\"\n",
    "    Computes Fisher's Exact Test, Odds Ratio, p-value, and Confidence Interval with Haldane-Anscombe correction.\n",
    "\n",
    "    Args:\n",
    "        a, b, c, d: Cell counts of 2x2 contingency table.\n",
    "        confidence_level: CI level (default = 95%).\n",
    "        alternative: 'greater' (default), 'less', or 'two-sided'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (odds_ratio, p_value, ci_low, ci_high)\n",
    "    \"\"\"\n",
    "    # Apply Haldane-Anscombe correction if any value is zero\n",
    "    if 0 in [a, b, c, d]:\n",
    "        a, b, c, d = a + 0.5, b + 0.5, c + 0.5, d + 0.5\n",
    "\n",
    "    # Construct contingency table\n",
    "    table = [[a, b], [c, d]]\n",
    "\n",
    "    # Compute Odds Ratio\n",
    "    try:\n",
    "        odds_ratio = (a * d) / (b * c)\n",
    "    except ZeroDivisionError:\n",
    "        odds_ratio = np.inf if a * d > 0 else 0\n",
    "\n",
    "    # Compute one-sided Fisher's Exact Test p-value\n",
    "    try:\n",
    "        _, p_value = stats.fisher_exact(table, alternative=alternative)\n",
    "    except:\n",
    "        p_value = np.nan\n",
    "\n",
    "    # Compute Confidence Interval using log method\n",
    "    try:\n",
    "        log_or = np.log(odds_ratio)\n",
    "        se_log_or = np.sqrt(1/a + 1/b + 1/c + 1/d)\n",
    "        z = stats.norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "        ci_low = np.exp(log_or - z * se_log_or)\n",
    "        ci_high = np.exp(log_or + z * se_log_or)\n",
    "    except:\n",
    "        ci_low, ci_high = np.nan, np.nan\n",
    "\n",
    "    # Handle extreme OR edge cases\n",
    "    if odds_ratio == 0:\n",
    "        ci_low, ci_high = 0, np.inf\n",
    "    elif np.isinf(odds_ratio):\n",
    "        ci_low, ci_high = 0, np.inf\n",
    "\n",
    "    return odds_ratio, p_value, (ci_low, ci_high)\n",
    "\n",
    "\n",
    "def process_csv(file_path, output_path=None, alternative='greater'):\n",
    "    \"\"\"\n",
    "    Processes a CSV file and applies Fisher's test on each row.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to input CSV.\n",
    "        output_path: Path to save output CSV (optional).\n",
    "        alternative: One-sided alternative hypothesis ('greater' or 'less').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with test results.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        project_name = row.get(\"Project Name\", f\"Project_{index + 1}\")\n",
    "\n",
    "        # Extract and safely convert contingency table values\n",
    "        a = pd.to_numeric(row.get(\"Both Smelly & Buggy\"), errors='coerce') or 0\n",
    "        b = pd.to_numeric(row.get(\"Smelly But Not Buggy\"), errors='coerce') or 0\n",
    "        c = pd.to_numeric(row.get(\"Buggy But Not Smelly\"), errors='coerce') or 0\n",
    "        d = pd.to_numeric(row.get(\"Neither\"), errors='coerce') or 0\n",
    "\n",
    "        if (a + b == 0) or (c + d == 0):\n",
    "            odds_ratio, p_value, ci = None, None, (None, None)\n",
    "        else:\n",
    "            odds_ratio, p_value, ci = fisher_test_with_adjustment(a, b, c, d, alternative=alternative)\n",
    "\n",
    "        results.append([\n",
    "            project_name, int(a), int(b), int(c), int(d),\n",
    "            odds_ratio, p_value, ci[0], ci[1]\n",
    "        ])\n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=[\n",
    "        \"Project\", \"a\", \"b\", \"c\", \"d\", \n",
    "        \"Odds Ratio\", \"p-value\", \"CI Lower\", \"CI Upper\"\n",
    "    ])\n",
    "\n",
    "    if output_path:\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        print(f\"Results saved to {output_path}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"buggy_smelly/revision/fisher/data_for_fault_analysis.csv\"\n",
    "    output_file = \"buggy_smelly/revision/fisher/results_for_fault_analysis.csv\"\n",
    "\n",
    "    df_result = process_csv(csv_file, output_file, alternative='two-sided')\n",
    "    print(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c54be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forest plot for fisher results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def create_grouped_forest_plot_two_colors(csv_file, title=\"Multi-language Design Smells: Fault-Proneness by Project\"):\n",
    "    \"\"\"Create grouped forest plot with ONLY red and gray markers\"\"\"\n",
    "    \n",
    "    # Read and preprocess data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[['system', 'release']] = df['Project'].str.split('-', n=1, expand=True)\n",
    "    plot_df = df.dropna(subset=['ci_lower', 'ci_upper']).copy()\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Get unique systems and create positions\n",
    "    systems = plot_df['system'].unique()\n",
    "    y_positions = np.arange(len(systems))\n",
    "    \n",
    "    # Plot each system - ONLY TWO COLORS\n",
    "    for i, system in enumerate(systems):\n",
    "        system_data = plot_df[plot_df['system'] == system]\n",
    "        \n",
    "        # Calculate jitter based on number of releases\n",
    "        n_releases = len(system_data)\n",
    "        if n_releases > 1:\n",
    "            jitter = np.random.normal(0, 0.08, len(system_data))  # Small random jitter\n",
    "        else:\n",
    "            jitter = [0]\n",
    "        \n",
    "        for j, (idx, row) in enumerate(system_data.iterrows()):\n",
    "            # Plot confidence interval as a vertical line (thicker)\n",
    "            ax.plot([row['ci_lower'], row['ci_upper']], \n",
    "                    [i + jitter[j], i + jitter[j]], \n",
    "                    color='gray', alpha=0.7, linewidth=1, zorder=1)\n",
    "            \n",
    "            # Plot odds ratio point - ONLY TWO COLORS\n",
    "            # MODIFIED: Significant only when p < 0.05 AND ci_lower > 1\n",
    "            is_significant = (row['p_value'] < 0.05) and (row['ci_lower'] > 1)\n",
    "            color = 'red' if is_significant else 'gray'  # ONLY RED AND GRAY\n",
    "            marker = 'D' if is_significant else 'o'\n",
    "            size = 120 if is_significant else 80  # Increased sizes\n",
    "            \n",
    "            ax.scatter(row['Odds Ratio'], i + jitter[j], \n",
    "                      color=color, s=size, marker=marker,\n",
    "                      edgecolor='black', alpha=0.9, zorder=2,\n",
    "                      linewidth=1.5)  # Thicker borders\n",
    "    \n",
    "    # Customize plot with FIXED RANGE\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels([f\"{sys} ({len(plot_df[plot_df['system']==sys])} releases)\" \n",
    "                       for sys in systems], fontsize=11)\n",
    "    ax.set_xlabel('Odds Ratio (log scale)', fontsize=13)\n",
    "    ax.set_ylabel('Project', fontsize=13)\n",
    "    ax.set_title(title, fontsize=16, pad=20)\n",
    "    ax.set_xscale('log')\n",
    "    \n",
    "    # SET FIXED X-AXIS RANGE: 10^-2 to 10^2\n",
    "    ax.set_xlim(0.01, 100)  # 10^-2 to 10^2\n",
    "    \n",
    "    # Set custom x-ticks for better readability\n",
    "    ax.set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "    ax.set_xticklabels(['0.01', '0.1', '1', '10', '100'])\n",
    "    \n",
    "    # Reference line\n",
    "    ax.axvline(x=1, color='red', linestyle='--', alpha=0.8, \n",
    "               label='No effect (OR=1)', linewidth=2.5)\n",
    "    \n",
    "    # Styling\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Enhanced legend with larger markers - UPDATED DESCRIPTION\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', \n",
    "               markersize=12, label='Not significant', markeredgecolor='black', markeredgewidth=1),\n",
    "        Line2D([0], [0], marker='D', color='w', markerfacecolor='red', \n",
    "               markersize=12, label='Significant (p < 0.05 & CI lower > 1)', markeredgecolor='black', markeredgewidth=1),\n",
    "        Line2D([0], [0], color='red', linestyle='--', label='No effect (OR=1)', linewidth=2)\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=11)\n",
    "    \n",
    "    # Add some statistics to the plot\n",
    "    total_releases = len(plot_df)\n",
    "    # MODIFIED: Count significant using the new criteria\n",
    "    significant_releases = len(plot_df[(plot_df['p_value'] < 0.05) & (plot_df['ci_lower'] > 1)])\n",
    "    \n",
    "#     # Add text box with statistics\n",
    "#     textstr = f'Total releases: {total_releases}\\nSignificant: {significant_releases} ({significant_releases/total_releases*100:.1f}%)'\n",
    "#     props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "#     ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n",
    "#             verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, plot_df\n",
    "\n",
    "# Usage\n",
    "fig, processed_df = create_grouped_forest_plot_two_colors(\"buggy_smelly/combined/fisher_results_fault_analysis.csv\")\n",
    "plt.savefig('buggy_smelly/combined/forest_plot_fault_analysis.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression (projectwise)\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming nativedata is already loaded as a pandas DataFrame\n",
    "nativedata = pd.read_csv(\"buggy_smelly/combined/fault LR data combined.csv\")\n",
    "output_path = \"buggy_smelly/combined/fault LR result combined.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Load and clean dataset\n",
    "df = nativedata.copy()\n",
    "\n",
    "features = [\n",
    "    'ASM', 'EILC', 'HCL', 'LRA', 'MMM', 'NCOE', 'NHE', 'NSL', 'NURP', 'PEO', 'TMC', 'TMS', 'UNMD', 'UNMI', 'UP',\n",
    "'loc', 'previous_fixes', 'code_churn'\n",
    "\n",
    "#     'excessiveInterlangCommunication', 'Toomuchclustring', 'ToomuchScattering',\n",
    "#     'UnusedMethodDeclaration', 'UnusedMethodImplementation', 'UnusedParameter',\n",
    "#     'AssumingSafeReturnValue', 'ExcessiveObjects', 'NotHandlingExceptions',\n",
    "#     'NotCachingObjects', 'NotSecuringLibraries', 'HardCodingLibraries',\n",
    "#     'NotUsingRelativePath', 'MemoryManagementMismatch', 'LocalReferencesAbuse',\n",
    "#     'LOC', 'PrevFixing', 'CodeChurn'\n",
    "]\n",
    "target = 'Changed'\n",
    "\n",
    "# Filter rows and columns\n",
    "X = df[features].dropna()\n",
    "y = df.loc[X.index, target]\n",
    "\n",
    "# Step 1: Drop zero-variance features\n",
    "X = X.loc[:, X.nunique() > 1]\n",
    "\n",
    "# Step 2: Drop one of highly correlated pairs\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "X = X.drop(columns=to_drop_corr)\n",
    "print(\"Dropped (correlation):\", to_drop_corr)\n",
    "\n",
    "# Step 3: Drop high-VIF variables iteratively\n",
    "def calculate_vif(X_):\n",
    "    X_vif = sm.add_constant(X_)\n",
    "    return pd.DataFrame({\n",
    "        \"feature\": X_vif.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "    })\n",
    "\n",
    "while True:\n",
    "    vif_df = calculate_vif(X)\n",
    "    vif_df = vif_df[vif_df['feature'] != 'const']\n",
    "    max_vif = vif_df['VIF'].max()\n",
    "    if max_vif > 10:\n",
    "        to_drop_vif = vif_df.sort_values('VIF', ascending=False).iloc[0]['feature']\n",
    "        print(f\"Dropped (VIF): {to_drop_vif}, VIF={max_vif:.2f}\")\n",
    "        X = X.drop(columns=[to_drop_vif])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Step 4: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "\n",
    "# Step 5: Use GLM instead of Logit (more stable), with max iterations increased\n",
    "model = sm.GLM(y, X_scaled, family=sm.families.Binomial())\n",
    "result = model.fit(maxiter=100, disp=0)\n",
    "\n",
    "# Step 6: Output results\n",
    "print(result.summary())\n",
    "# Step 6: Output results\n",
    "#print(result.summary())\n",
    "\n",
    "# Step 7: Compute and display Odds Ratios with 95% CI\n",
    "odds_ratios = np.exp(result.params)\n",
    "conf = result.conf_int()\n",
    "conf['OR_lower'] = np.exp(conf[0])\n",
    "conf['OR_upper'] = np.exp(conf[1])\n",
    "odds_ratios_df = pd.DataFrame({\n",
    "    'Odds Ratio': odds_ratios,\n",
    "    'CI Lower': conf['OR_lower'],\n",
    "    'CI Upper': conf['OR_upper'],\n",
    "    'p-value': result.pvalues\n",
    "})\n",
    "\n",
    "print(\"\\n=== Odds Ratios with 95% CI ===\")\n",
    "print(odds_ratios_df)\n",
    "\n",
    "# Step 7: Prepare results with guaranteed row alignment\n",
    "index = result.params.index  # ensures 'const' and all variable names are preserved\n",
    "\n",
    "summary_df = pd.DataFrame(index=index)\n",
    "summary_df['Coefficient'] = result.params\n",
    "summary_df['Odds Ratio'] = np.exp(result.params)\n",
    "summary_df['p-value'] = result.pvalues\n",
    "\n",
    "conf = result.conf_int().rename(columns={0: 'CI Lower', 1: 'CI Upper'})\n",
    "conf['OR Lower'] = np.exp(conf['CI Lower'])\n",
    "conf['OR Upper'] = np.exp(conf['CI Upper'])\n",
    "summary_df['95% CI'] = conf.apply(lambda row: f\"({row['OR Lower']:.4f}, {row['OR Upper']:.4f})\", axis=1)\n",
    "\n",
    "# Step 8: Write to CSV\n",
    "summary_df.index.name = 'Variable'\n",
    "\n",
    "summary_df.to_csv(output_path)\n",
    "print(f\"\\nâœ… All results written to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7aaa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR for pooled analysis for change-proneness\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_mixed_effects_analysis_change(nativedata, target='Changed'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df = nativedata.copy()\n",
    "    \n",
    "    # Basic features (only design smells, no additional controls)\n",
    "    features = [\n",
    "        'ASM', 'EILC', 'HCL', 'LRA', 'MMM', 'NCOE', 'NHE', 'NSL', 'NURP', \n",
    "        'PEO', 'TMC', 'TMS', 'UNMD', 'UNMI', 'UP'\n",
    "    ]\n",
    "    \n",
    "    # Add project identifier\n",
    "    df['project_name'] = df['Project'].str.split('-').str[0]\n",
    "    \n",
    "    # Filter and prepare data\n",
    "    X = df[features + ['project_name']].dropna()\n",
    "    y = df.loc[X.index, target]\n",
    "    \n",
    "    print(f\"Total observations: {len(X)}\")\n",
    "    print(f\"Projects: {X['project_name'].unique()}\")\n",
    "    print(f\"Observations per project:\")\n",
    "    print(X['project_name'].value_counts())\n",
    "    \n",
    "    # Since statsmodels doesn't have built-in mixed logit, we'll use fixed effects with project dummies\n",
    "    # and cluster standard errors by project\n",
    "    X_fixed = X.drop('project_name', axis=1)\n",
    "    \n",
    "    # Add project fixed effects - ensure they're numeric\n",
    "    project_dummies = pd.get_dummies(X['project_name'], prefix='project', drop_first=True)\n",
    "    \n",
    "    # Convert all dummies to int (they might be bool)\n",
    "    project_dummies = project_dummies.astype(int)\n",
    "    \n",
    "    X_fixed = pd.concat([X_fixed, project_dummies], axis=1)\n",
    "    \n",
    "    # Convert ALL columns to numeric to be safe\n",
    "    for col in X_fixed.columns:\n",
    "        X_fixed[col] = pd.to_numeric(X_fixed[col], errors='coerce')\n",
    "    \n",
    "    # Drop any new NaN values created during conversion\n",
    "    X_fixed = X_fixed.dropna()\n",
    "    \n",
    "    # Align y with X_fixed after dropping NaN\n",
    "    y = y.loc[X_fixed.index]\n",
    "    \n",
    "    # No standardization needed since we're only using count variables (design smells)\n",
    "    # All smell variables are already on similar scales (counts/frequencies)\n",
    "    \n",
    "    X_fixed = sm.add_constant(X_fixed)\n",
    "    \n",
    "    print(f\"Final X shape: {X_fixed.shape}\")\n",
    "    print(f\"Final X dtypes: {X_fixed.dtypes.unique()}\")\n",
    "    \n",
    "    # Fit model with clustered standard errors by project\n",
    "    model = sm.GLM(y, X_fixed, family=sm.families.Binomial())\n",
    "    result = model.fit(cov_type='cluster', cov_kwds={'groups': X.loc[X_fixed.index, 'project_name']}, maxiter=100)\n",
    "    \n",
    "    return result, X_fixed, X.loc[X_fixed.index, 'project_name']\n",
    "\n",
    "def save_results_to_csv(result, output_path):\n",
    "    \"\"\"\n",
    "    Save comprehensive regression results to CSV\n",
    "    \"\"\"\n",
    "    # Create comprehensive results dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        'Variable': result.params.index,\n",
    "        'Coefficient': result.params.values,\n",
    "        'Std_Error': result.bse.values,\n",
    "        'z_Value': result.tvalues.values,\n",
    "        'P_Value': result.pvalues.values,\n",
    "        'Odds_Ratio': np.exp(result.params.values),\n",
    "        'CI_Lower': np.exp(result.conf_int()[0].values),\n",
    "        'CI_Upper': np.exp(result.conf_int()[1].values),\n",
    "        'Significant_0.05': result.pvalues.values < 0.05,\n",
    "        'Significant_0.01': result.pvalues.values < 0.01\n",
    "    })\n",
    "    \n",
    "    # Add significance stars\n",
    "    def add_significance_stars(p_value):\n",
    "        if p_value < 0.001:\n",
    "            return '***'\n",
    "        elif p_value < 0.01:\n",
    "            return '**'\n",
    "        elif p_value < 0.05:\n",
    "            return '*'\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    results_df['Significance'] = results_df['P_Value'].apply(add_significance_stars)\n",
    "    \n",
    "    # Format confidence interval as string\n",
    "    results_df['95%_CI'] = results_df.apply(\n",
    "        lambda row: f\"[{row['CI_Lower']:.4f}, {row['CI_Upper']:.4f}]\", axis=1\n",
    "    )\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    final_columns = [\n",
    "        'Variable', 'Coefficient', 'Std_Error', 'z_Value', 'P_Value', \n",
    "        'Significance', 'Odds_Ratio', '95%_CI', 'CI_Lower', 'CI_Upper',\n",
    "        'Significant_0.05', 'Significant_0.01'\n",
    "    ]\n",
    "    \n",
    "    results_df = results_df[final_columns]\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nâœ… Comprehensive results saved to: {output_path}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run the analysis for change-proneness\n",
    "nativedata = pd.read_csv(\"buggy_smelly/combined/change LR data combined.csv\")\n",
    "result_mixed, X_mixed, projects = run_mixed_effects_analysis_change(nativedata, target='Changed')\n",
    "\n",
    "print(\"=== MIXED EFFECTS MODEL RESULTS FOR CHANGE-PRONENESS ===\")\n",
    "print(\"(With project clustering, design smells only - no additional controls)\")\n",
    "print(result_mixed.summary())\n",
    "\n",
    "# Extract and display key results\n",
    "coef_df = pd.DataFrame({\n",
    "    'Coefficient': result_mixed.params,\n",
    "    'Odds_Ratio': np.exp(result_mixed.params),\n",
    "    'P_Value': result_mixed.pvalues,\n",
    "    'CI_Lower': np.exp(result_mixed.conf_int()[0]),\n",
    "    'CI_Upper': np.exp(result_mixed.conf_int()[1])\n",
    "})\n",
    "\n",
    "print(\"\\n=== KEY DESIGN SMELL PREDICTORS (excluding project dummies) ===\")\n",
    "key_predictors = coef_df.loc[~coef_df.index.str.startswith('project_') & \n",
    "                            ~coef_df.index.str.startswith('const')]\n",
    "print(key_predictors.sort_values('P_Value'))\n",
    "\n",
    "# Save results to CSV\n",
    "output_path = \"buggy_smelly/combined/combined_change_proneness_LR_mixed_effects_results.csv\"\n",
    "saved_results = save_results_to_csv(result_mixed, output_path)\n",
    "\n",
    "# Also save a simplified version with only design smells (excluding project dummies)\n",
    "print(\"\\n=== SAVING SIMPLIFIED RESULTS (Design Smells Only) ===\")\n",
    "design_smells_results = saved_results[~saved_results['Variable'].str.startswith('project_') & \n",
    "                                     ~saved_results['Variable'].str.startswith('const')]\n",
    "design_smells_output_path = \"buggy_smelly/combined/combined_change_proneness_design_smells_only.csv\"\n",
    "design_smells_results.to_csv(design_smells_output_path, index=False)\n",
    "print(f\"âœ… Design smells results saved to: {design_smells_output_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== MODEL SUMMARY FOR CHANGE-PRONENESS ===\")\n",
    "print(f\"Number of observations: {result_mixed.nobs}\")\n",
    "print(f\"Log-Likelihood: {result_mixed.llf:.2f}\")\n",
    "print(f\"Pseudo R-squared: {result_mixed.pseudo_rsquared():.4f}\")\n",
    "print(f\"Significant design smells (p < 0.05): {sum(design_smells_results['Significant_0.05'])}\")\n",
    "print(f\"Significant design smells (p < 0.01): {sum(design_smells_results['Significant_0.01'])}\")\n",
    "\n",
    "# Display the most significant design smells\n",
    "print(\"\\n=== TOP 10 MOST SIGNIFICANT DESIGN SMELLS FOR CHANGE-PRONENESS ===\")\n",
    "top_smells = design_smells_results.nsmallest(10, 'P_Value')[['Variable', 'Coefficient', 'Odds_Ratio', 'P_Value', 'Significance']]\n",
    "print(top_smells.to_string(index=False))\n",
    "\n",
    "# Additional: Save model summary as text\n",
    "def save_model_summary(result, output_path):\n",
    "    \"\"\"\n",
    "    Save the raw model summary as text file\n",
    "    \"\"\"\n",
    "    summary_text = result.summary().as_text()\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    \n",
    "    print(f\"âœ… Model summary saved to: {output_path}\")\n",
    "\n",
    "# Save raw model summary\n",
    "summary_output_path = \"buggy_smelly/combined/change_proneness_model_summary.txt\"\n",
    "save_model_summary(result_mixed, summary_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR for pooled analysis with control variables for fault-analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_mixed_effects_analysis_fixed(nativedata, target='Changed'):\n",
    "    \"\"\"\n",
    "    Fixed version - ensures all data is numeric\n",
    "    \"\"\"\n",
    "    df = nativedata.copy()\n",
    "    \n",
    "    # Basic features (smells + controls)\n",
    "    features = [\n",
    "        'ASM', 'EILC', 'HCL', 'LRA', 'MMM', 'NCOE', 'NHE', 'NSL', 'NURP', \n",
    "        'PEO', 'TMC', 'TMS', 'UNMD', 'UNMI', 'UP', 'loc', 'previous_fixes', 'code_churn'\n",
    "    ]\n",
    "    \n",
    "    # Add project identifier\n",
    "    df['project_name'] = df['Project'].str.split('-').str[0]\n",
    "    \n",
    "    # Filter and prepare data\n",
    "    X = df[features + ['project_name']].dropna()\n",
    "    y = df.loc[X.index, target]\n",
    "    \n",
    "    print(f\"Total observations: {len(X)}\")\n",
    "    print(f\"Projects: {X['project_name'].unique()}\")\n",
    "    \n",
    "    # Since statsmodels doesn't have built-in mixed logit, we'll use fixed effects with project dummies\n",
    "    # and cluster standard errors by project\n",
    "    X_fixed = X.drop('project_name', axis=1)\n",
    "    \n",
    "    # Add project fixed effects - ensure they're numeric\n",
    "    project_dummies = pd.get_dummies(X['project_name'], prefix='project', drop_first=True)\n",
    "    \n",
    "    # Convert all dummies to int (they might be bool)\n",
    "    project_dummies = project_dummies.astype(int)\n",
    "    \n",
    "    X_fixed = pd.concat([X_fixed, project_dummies], axis=1)\n",
    "    \n",
    "    # Convert ALL columns to numeric to be safe\n",
    "    for col in X_fixed.columns:\n",
    "        X_fixed[col] = pd.to_numeric(X_fixed[col], errors='coerce')\n",
    "    \n",
    "    # Drop any new NaN values created during conversion\n",
    "    X_fixed = X_fixed.dropna()\n",
    "    \n",
    "    # Align y with X_fixed after dropping NaN\n",
    "    y = y.loc[X_fixed.index]\n",
    "    \n",
    "    # Standardize continuous features (excluding dummies)\n",
    "    continuous_features = ['loc', 'previous_fixes', 'code_churn']\n",
    "    scaler = StandardScaler()\n",
    "    X_fixed[continuous_features] = scaler.fit_transform(X_fixed[continuous_features])\n",
    "    \n",
    "    X_fixed = sm.add_constant(X_fixed)\n",
    "    \n",
    "    print(f\"Final X shape: {X_fixed.shape}\")\n",
    "    print(f\"Final X dtypes: {X_fixed.dtypes.unique()}\")\n",
    "    \n",
    "    # Fit model with clustered standard errors by project\n",
    "    model = sm.GLM(y, X_fixed, family=sm.families.Binomial())\n",
    "    result = model.fit(cov_type='cluster', cov_kwds={'groups': X.loc[X_fixed.index, 'project_name']}, maxiter=100)\n",
    "    \n",
    "    return result, X_fixed, X.loc[X_fixed.index, 'project_name']\n",
    "\n",
    "def save_results_to_csv(result, output_path):\n",
    "    \"\"\"\n",
    "    Save comprehensive regression results to CSV\n",
    "    \"\"\"\n",
    "    # Create comprehensive results dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        'Variable': result.params.index,\n",
    "        'Coefficient': result.params.values,\n",
    "        'Std_Error': result.bse.values,\n",
    "        'z_Value': result.tvalues.values,\n",
    "        'P_Value': result.pvalues.values,\n",
    "        'Odds_Ratio': np.exp(result.params.values),\n",
    "        'CI_Lower': np.exp(result.conf_int()[0].values),\n",
    "        'CI_Upper': np.exp(result.conf_int()[1].values),\n",
    "        'Significant_0.05': result.pvalues.values < 0.05,\n",
    "        'Significant_0.01': result.pvalues.values < 0.01\n",
    "    })\n",
    "    \n",
    "    # Add significance stars\n",
    "    def add_significance_stars(p_value):\n",
    "        if p_value < 0.001:\n",
    "            return '***'\n",
    "        elif p_value < 0.01:\n",
    "            return '**'\n",
    "        elif p_value < 0.05:\n",
    "            return '*'\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    results_df['Significance'] = results_df['P_Value'].apply(add_significance_stars)\n",
    "    \n",
    "    # Format confidence interval as string\n",
    "    results_df['95%_CI'] = results_df.apply(\n",
    "        lambda row: f\"[{row['CI_Lower']:.4f}, {row['CI_Upper']:.4f}]\", axis=1\n",
    "    )\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    final_columns = [\n",
    "        'Variable', 'Coefficient', 'Std_Error', 'z_Value', 'P_Value', \n",
    "        'Significance', 'Odds_Ratio', '95%_CI', 'CI_Lower', 'CI_Upper',\n",
    "        'Significant_0.05', 'Significant_0.01'\n",
    "    ]\n",
    "    \n",
    "    results_df = results_df[final_columns]\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nâœ… Comprehensive results saved to: {output_path}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run the analysis\n",
    "nativedata = pd.read_csv(\"buggy_smelly/combined/fault LR data combined.csv\")\n",
    "result_mixed, X_mixed, projects = run_mixed_effects_analysis_fixed(nativedata, target='Changed')\n",
    "\n",
    "print(\"=== MIXED EFFECTS MODEL RESULTS (with project clustering) ===\")\n",
    "print(result_mixed.summary())\n",
    "\n",
    "# Extract and display key results\n",
    "coef_df = pd.DataFrame({\n",
    "    'Coefficient': result_mixed.params,\n",
    "    'Odds_Ratio': np.exp(result_mixed.params),\n",
    "    'P_Value': result_mixed.pvalues,\n",
    "    'CI_Lower': np.exp(result_mixed.conf_int()[0]),\n",
    "    'CI_Upper': np.exp(result_mixed.conf_int()[1])\n",
    "})\n",
    "\n",
    "print(\"\\n=== KEY PREDICTORS (excluding project dummies) ===\")\n",
    "key_predictors = coef_df.loc[~coef_df.index.str.startswith('project_') & \n",
    "                            ~coef_df.index.str.startswith('const')]\n",
    "print(key_predictors.sort_values('P_Value'))\n",
    "\n",
    "# Save results to CSV\n",
    "output_path = \"buggy_smelly/combined/combined_fault_LR_mixed_effects_results.csv\"\n",
    "saved_results = save_results_to_csv(result_mixed, output_path)\n",
    "\n",
    "# Also save a simplified version with only key predictors (excluding project dummies)\n",
    "print(\"\\n=== SAVING SIMPLIFIED RESULTS (Key Predictors Only) ===\")\n",
    "key_results = saved_results[~saved_results['Variable'].str.startswith('project_') & \n",
    "                           ~saved_results['Variable'].str.startswith('const')]\n",
    "key_output_path = \"buggy_smelly/combined/combined_fault_LR_key_predictors.csv\"\n",
    "key_results.to_csv(key_output_path, index=False)\n",
    "print(f\"âœ… Key predictors saved to: {key_output_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== MODEL SUMMARY ===\")\n",
    "print(f\"Number of observations: {result_mixed.nobs}\")\n",
    "print(f\"Log-Likelihood: {result_mixed.llf:.2f}\")\n",
    "print(f\"Pseudo R-squared: {result_mixed.pseudo_rsquared():.4f}\")\n",
    "print(f\"Significant predictors (p < 0.05): {sum(saved_results['Significant_0.05'])}\")\n",
    "print(f\"Significant predictors (p < 0.01): {sum(saved_results['Significant_0.01'])}\")\n",
    "\n",
    "# Display the most significant results\n",
    "print(\"\\n=== TOP 10 MOST SIGNIFICANT PREDICTORS ===\")\n",
    "top_predictors = saved_results.nsmallest(10, 'P_Value')[['Variable', 'Coefficient', 'Odds_Ratio', 'P_Value', 'Significance']]\n",
    "print(top_predictors.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
